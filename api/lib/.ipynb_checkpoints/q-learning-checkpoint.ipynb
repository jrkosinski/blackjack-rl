{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "790b1fa2-d3b7-4e66-bb4a-c84e6c0e3468",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBlackjack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Table, Dealer, Player, Shoe, DecisionModel\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow import keras\n",
    "\n",
    "from lib.Blackjack import Table, Dealer, Player, Shoe, DecisionModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11f53c6-0a07-4331-bc17-4dc996796f75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebc3df3f-609b-45ec-a32e-65425b152f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "        \n",
    "class DQNAgent: \n",
    "    def __init__(self, state_size, action_size, epsilon):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.epsilon = epsilon\n",
    "        self.model = self.build_model()\n",
    "        self.replay_buffer = ReplayBuffer(10000)\n",
    "        \n",
    "    def build_model(self):\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(24, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam())\n",
    "        return model\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            #explore: select a random action\n",
    "            return np.random.choice(2)\n",
    "        else:\n",
    "            #exploit: select the action with max Q-value\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            q_values = self.model(state_tensor)\n",
    "            return np.argmax(q_values[0].numpy())\n",
    "    \n",
    "    def update_epsilon(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def update_model(self, batch_size, gamma=0.5):\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n",
    "\n",
    "        state_tensor = tf.convert_to_tensor(state)\n",
    "        next_state_tensor = tf.convert_to_tensor(next_state)\n",
    "        \n",
    "        q_values = self.model(state_tensor)\n",
    "        q_values_next = self.model(next_state_tensor)\n",
    "\n",
    "        q_target = reward + gamma * tf.reduce_max(q_values_next, axis=1)\n",
    "        q_target = tf.where(done, reward, q_target)\n",
    "        \n",
    "        indices = tf.range(batch_size)[:, tf.newaxis]\n",
    "        actions = tf.cast(action, dtype=tf.int32)[:, tf.newaxis]\n",
    "        indices_actions = tf.concat([indices, actions], axis=1)\n",
    "        \n",
    "        q_values = tf.tensor_scatter_nd_update(q_values, indices_actions, q_target)\n",
    "\n",
    "        self.model.fit(state_tensor, q_values, epochs=1, verbose=0)\n",
    "\n",
    "    def save_model(self): \n",
    "        self.model.save_weights(\"model/model_saved\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e11b6e6-c4f0-45be-8e3c-88f4597f4e42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DecisionModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mQLearningDecisionModel\u001b[39;00m(\u001b[43mDecisionModel\u001b[49m): \n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent: DQNAgent): \n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent \u001b[38;5;241m=\u001b[39m agent\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DecisionModel' is not defined"
     ]
    }
   ],
   "source": [
    "      \n",
    "class QLearningDecisionModel(DecisionModel): \n",
    "    def __init__(self, agent: DQNAgent): \n",
    "        self.agent = agent\n",
    "        \n",
    "    def decide_hit(self, dealer: Dealer, shoe: Shoe, players, player_index: int):\n",
    "        soft_ace_count = players[player_index].hand.soft_ace_count\n",
    "        if (soft_ace_count > 2): \n",
    "            soft_ace_count = 2\n",
    "            \n",
    "        state = [\n",
    "            dealer.showing, \n",
    "            players[player_index].hand.total,\n",
    "            soft_ace_count\n",
    "        ]\n",
    "        \n",
    "        return self.agent.get_action(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "520a211d-058b-4949-abfc-5f1a780a5694",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BlackjackEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m epsilon_decay_duration \u001b[38;5;241m=\u001b[39m num_episodes \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Environment and agent\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mBlackjackEnv\u001b[49m(deck_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, hand_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     10\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(state_size\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mstate_size, action_size\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39maction_size, epsilon\u001b[38;5;241m=\u001b[39mstart_epsilon)\n\u001b[1;32m     11\u001b[0m replay_buffer \u001b[38;5;241m=\u001b[39m ReplayBuffer(\u001b[38;5;241m10000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BlackjackEnv' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class TrainingEpisode: \n",
    "    def __init__(self, batch_size: int, num_decks: int, agent: DQNAgent): \n",
    "        self.agent = agent\n",
    "        self.batch_size = batch_size\n",
    "        self.player = Player(QLearningDecisionModel(agent))\n",
    "        self.table = Table(Dealer(), num_decks)\n",
    "        self.table.add_player(self.player)\n",
    "        self.prev_state = None\n",
    "        \n",
    "    def run(self): \n",
    "        self.table.deal_hands()\n",
    "        \n",
    "        self.prev_state = self._get_current_state()\n",
    "        player_start_balance = self.player.balance\n",
    "        \n",
    "        def on_game_action(player: Player, done: bool): \n",
    "            next_state = self._get_current_state()\n",
    "            reward = player.balance - player_start_balance\n",
    "            self.agent.replay_buffer.push(self.prev_state, player.last_action, reward, next_state, done)\n",
    "            self.prev_state = next_state\n",
    "            \n",
    "            self.agent.update_model(self.batch_size)\n",
    "            \n",
    "        self.table.on_action(on_game_action)\n",
    "        self.table.dealer.play_round(self.table.shoe, self.table.players, on_game_action)\n",
    "        self.table.dealer.assess_winners(self.table.players, on_game_action)\n",
    "    \n",
    "    def _get_current_state(self): \n",
    "        #TODO: this code is duplicated \n",
    "        soft_ace_count = self.player.hand.soft_ace_count\n",
    "        if (soft_ace_count > 2): \n",
    "            soft_ace_count = 2\n",
    "            \n",
    "        state = [\n",
    "            self.table.dealer.showing, \n",
    "            self.player.hand.total,\n",
    "            soft_ace_count\n",
    "        ]\n",
    "        \n",
    "        return state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37d135e-08f8-44bc-b147-dc145015349b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
